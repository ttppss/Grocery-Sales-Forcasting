{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SalesPrediction1026\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays_events = spark.read.csv(\"holidays_events.csv\", header=True)\n",
    "items = spark.read.csv(\"items.csv\", header=True)\n",
    "oil = spark.read.csv(\"oil.csv\", header=True)\n",
    "stores = spark.read.csv(\"stores.csv\", header=True)\n",
    "test = spark.read.csv(\"test.csv\", header=True)\n",
    "train = spark.read.csv(\"train.csv\", header=True)\n",
    "transactions = spark.read.csv(\"transactions.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deal with oil dataframe in pandas with backfill, then transform it to spark dataframe\n",
    "oil_pandas = pd.read_csv(\"oil.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check every dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+----------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|\n",
      "+---+----------+---------+--------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125497040"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|      date|   type|  locale|locale_name|         description|transferred|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02|Holiday|   Local|      Manta|  Fundacion de Manta|      False|\n",
      "|2012-04-01|Holiday|Regional|   Cotopaxi|Provincializacion...|      False|\n",
      "+----------+-------+--------+-----------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holidays_events.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+----------+\n",
      "|item_nbr|   family|class|perishable|\n",
      "+--------+---------+-----+----------+\n",
      "|   96995|GROCERY I| 1093|         0|\n",
      "|   99197|GROCERY I| 1067|         0|\n",
      "+--------+---------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|      null|\n",
      "|2013-01-02|     93.14|\n",
      "+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oil.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+----+-------+\n",
      "|store_nbr| city|    state|type|cluster|\n",
      "+---------+-----+---------+----+-------+\n",
      "|        1|Quito|Pichincha|   D|     13|\n",
      "|        2|Quito|Pichincha|   D|     13|\n",
      "+---------+-----+---------+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|      date|store_nbr|transactions|\n",
      "+----------+---------+------------+\n",
      "|2013-01-01|       25|         770|\n",
      "|2013-01-02|        1|        2111|\n",
      "+----------+---------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------+-----------+\n",
      "|       id|      date|store_nbr|item_nbr|onpromotion|\n",
      "+---------+----------+---------+--------+-----------+\n",
      "|125497040|2017-08-16|        1|   96995|      False|\n",
      "|125497041|2017-08-16|        1|   99197|      False|\n",
      "+---------+----------+---------+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='0', date='2013-01-01', store_nbr='25', item_nbr='103665', unit_sales='7.0', onpromotion=None),\n",
       " Row(id='1', date='2013-01-01', store_nbr='25', item_nbr='105574', unit_sales='1.0', onpromotion=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train and holiday_events dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|   type|  locale|locale_name|       description|transferred|\n",
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|\n",
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_holiday = train.join(holidays_events, train.date == holidays_events.date, 'left_outer').drop(holidays_events.date)\n",
    "train_holiday.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Oil Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01         NaN\n",
       "1  2013-01-02       93.14\n",
       "2  2013-01-03       92.97\n",
       "3  2013-01-04       93.12\n",
       "4  2013-01-07       93.20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date           0\n",
       "dcoilwtico    43\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01       93.14\n",
       "1  2013-01-02       93.14\n",
       "2  2013-01-03       92.97\n",
       "3  2013-01-04       93.12\n",
       "4  2013-01-07       93.20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_pandas = oil_pandas.fillna(method='bfill')\n",
    "oil_pandas = oil_pandas.fillna(method='ffill')\n",
    "oil_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          0\n",
       "dcoilwtico    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform pandas dataframe back to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|     93.14|\n",
      "|2013-01-02|     93.14|\n",
      "|2013-01-03|     92.97|\n",
      "|2013-01-04|     93.12|\n",
      "|2013-01-07|      93.2|\n",
      "|2013-01-08|     93.21|\n",
      "|2013-01-09|     93.08|\n",
      "|2013-01-10|     93.81|\n",
      "|2013-01-11|      93.6|\n",
      "|2013-01-14|     94.27|\n",
      "|2013-01-15|     93.26|\n",
      "|2013-01-16|     94.28|\n",
      "|2013-01-17|     95.49|\n",
      "|2013-01-18|     95.61|\n",
      "|2013-01-21|     96.09|\n",
      "|2013-01-22|     96.09|\n",
      "|2013-01-23|     95.06|\n",
      "|2013-01-24|     95.35|\n",
      "|2013-01-25|     95.15|\n",
      "|2013-01-28|     95.95|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oil_spark = spark.createDataFrame(oil_pandas)\n",
    "oil_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train with oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+\n",
      "|     id|      date|store_nbr|item_nbr|unit_sales|onpromotion|type|locale|locale_name|description|transferred|dcoilwtico|\n",
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+\n",
      "|2924449|2013-03-14|        1|  103520|       2.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924450|2013-03-14|        1|  103665|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924451|2013-03-14|        1|  105574|       2.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924452|2013-03-14|        1|  105575|       8.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924453|2013-03-14|        1|  105577|       5.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924454|2013-03-14|        1|  105737|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924455|2013-03-14|        1|  105857|       8.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924456|2013-03-14|        1|  106716|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924457|2013-03-14|        1|  108696|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924458|2013-03-14|        1|  108698|       2.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924459|2013-03-14|        1|  108701|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924460|2013-03-14|        1|  108786|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924461|2013-03-14|        1|  108831|     7.529|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924462|2013-03-14|        1|  108952|       2.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924463|2013-03-14|        1|  111223|       5.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924464|2013-03-14|        1|  112830|       2.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924465|2013-03-14|        1|  114778|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924466|2013-03-14|        1|  114790|       1.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924467|2013-03-14|        1|  115611|      17.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "|2924468|2013-03-14|        1|  115720|       3.0|       null|null|  null|       null|       null|       null|     93.03|\n",
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_holiday_oil = train_holiday.join(oil_spark, train.date == oil_spark.date, 'left_outer').drop(oil_spark.date)\n",
    "train_holiday_oil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+----------+\n",
      "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|   type|  locale|locale_name|       description|transferred|dcoilwtico|\n",
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+----------+\n",
      "|  0|2013-01-01|       25|  103665|       7.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  1|2013-01-01|       25|  105574|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  2|2013-01-01|       25|  105575|       2.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  3|2013-01-01|       25|  108079|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  4|2013-01-01|       25|  108701|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  5|2013-01-01|       25|  108786|       3.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  6|2013-01-01|       25|  108797|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  7|2013-01-01|       25|  108952|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  8|2013-01-01|       25|  111397|      13.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "|  9|2013-01-01|       25|  114790|       3.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 10|2013-01-01|       25|  114800|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 11|2013-01-01|       25|  115267|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 12|2013-01-01|       25|  115611|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 13|2013-01-01|       25|  115693|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 14|2013-01-01|       25|  115720|       5.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 15|2013-01-01|       25|  115850|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 16|2013-01-01|       25|  115891|       6.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 17|2013-01-01|       25|  115892|      10.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 18|2013-01-01|       25|  115894|       5.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "| 19|2013-01-01|       25|  119024|       1.0|       null|Holiday|National|    Ecuador|Primer dia del ano|      False|     93.14|\n",
      "+---+----------+---------+--------+----------+-----------+-------+--------+-----------+------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select date = 2013-03-01 see if it's in the dataframe\n",
    "train_holiday_oil.createOrReplaceTempView(\"train_holiday_oil_sql\")\n",
    "spark.sql(\"\"\"\n",
    "        SELECT * FROM\n",
    "        train_holiday_oil_sql t\n",
    "        WHERE date = '2013-01-01'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train_holiday_oil with store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename store.type so it would not duplicate with holiday.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----------+-------+\n",
      "|store_nbr|         city|               state|store_type|cluster|\n",
      "+---------+-------------+--------------------+----------+-------+\n",
      "|        1|        Quito|           Pichincha|         D|     13|\n",
      "|        2|        Quito|           Pichincha|         D|     13|\n",
      "|        3|        Quito|           Pichincha|         D|      8|\n",
      "|        4|        Quito|           Pichincha|         D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|         D|      4|\n",
      "|        6|        Quito|           Pichincha|         D|     13|\n",
      "|        7|        Quito|           Pichincha|         D|      8|\n",
      "|        8|        Quito|           Pichincha|         D|      8|\n",
      "|        9|        Quito|           Pichincha|         B|      6|\n",
      "|       10|        Quito|           Pichincha|         C|     15|\n",
      "|       11|      Cayambe|           Pichincha|         B|      6|\n",
      "|       12|    Latacunga|            Cotopaxi|         C|     15|\n",
      "|       13|    Latacunga|            Cotopaxi|         C|     15|\n",
      "|       14|     Riobamba|          Chimborazo|         C|      7|\n",
      "|       15|       Ibarra|            Imbabura|         C|     15|\n",
      "|       16|Santo Domingo|Santo Domingo de ...|         C|      3|\n",
      "|       17|        Quito|           Pichincha|         C|     12|\n",
      "|       18|        Quito|           Pichincha|         B|     16|\n",
      "|       19|     Guaranda|             Bolivar|         C|     15|\n",
      "|       20|        Quito|           Pichincha|         B|      6|\n",
      "+---------+-------------+--------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores = stores.withColumnRenamed(\"type\", \"store_type\")\n",
    "stores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+\n",
      "|     id|      date|store_nbr|item_nbr|unit_sales|onpromotion|type|locale|locale_name|description|transferred|dcoilwtico| city|    state|store_type|cluster|\n",
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+\n",
      "|2924449|2013-03-14|        1|  103520|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924450|2013-03-14|        1|  103665|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924451|2013-03-14|        1|  105574|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924452|2013-03-14|        1|  105575|       8.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924453|2013-03-14|        1|  105577|       5.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924454|2013-03-14|        1|  105737|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924455|2013-03-14|        1|  105857|       8.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924456|2013-03-14|        1|  106716|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924457|2013-03-14|        1|  108696|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924458|2013-03-14|        1|  108698|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924459|2013-03-14|        1|  108701|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924460|2013-03-14|        1|  108786|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924461|2013-03-14|        1|  108831|     7.529|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924462|2013-03-14|        1|  108952|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924463|2013-03-14|        1|  111223|       5.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924464|2013-03-14|        1|  112830|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924465|2013-03-14|        1|  114778|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924466|2013-03-14|        1|  114790|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924467|2013-03-14|        1|  115611|      17.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "|2924468|2013-03-14|        1|  115720|       3.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|\n",
      "+-------+----------+---------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_holiday_oil_store = train_holiday_oil.join(stores, train_holiday_oil.store_nbr == stores.store_nbr, 'left_outer').drop(stores.store_nbr)\n",
    "train_holiday_oil_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|      date|store_nbr|transactions|\n",
      "+----------+---------+------------+\n",
      "|2013-01-01|       25|         770|\n",
      "|2013-01-02|        1|        2111|\n",
      "|2013-01-02|        2|        2358|\n",
      "|2013-01-02|        3|        3487|\n",
      "|2013-01-02|        4|        1922|\n",
      "|2013-01-02|        5|        1903|\n",
      "|2013-01-02|        6|        2143|\n",
      "|2013-01-02|        7|        1874|\n",
      "|2013-01-02|        8|        3250|\n",
      "|2013-01-02|        9|        2940|\n",
      "|2013-01-02|       10|        1293|\n",
      "|2013-01-02|       11|        3547|\n",
      "|2013-01-02|       12|        1362|\n",
      "|2013-01-02|       13|        1102|\n",
      "|2013-01-02|       14|        2002|\n",
      "|2013-01-02|       15|        1622|\n",
      "|2013-01-02|       16|        1167|\n",
      "|2013-01-02|       17|        1580|\n",
      "|2013-01-02|       18|        1635|\n",
      "|2013-01-02|       19|        1369|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train_holiday_oil_store and transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try to avoid the duplicated column name:\n",
    "train_holiday_oil_store_transaction_combine = train_holiday_oil_store.join(transactions, ['date', 'store_nbr'], 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+------------+\n",
      "|      date|store_nbr|     id|item_nbr|unit_sales|onpromotion|type|locale|locale_name|description|transferred|dcoilwtico| city|    state|store_type|cluster|transactions|\n",
      "+----------+---------+-------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+------------+\n",
      "|2013-03-14|        1|2924449|  103520|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924450|  103665|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924451|  105574|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924452|  105575|       8.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924453|  105577|       5.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924454|  105737|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924455|  105857|       8.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924456|  106716|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924457|  108696|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924458|  108698|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924459|  108701|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924460|  108786|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924461|  108831|     7.529|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924462|  108952|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924463|  111223|       5.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924464|  112830|       2.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924465|  114778|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924466|  114790|       1.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924467|  115611|      17.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "|2013-03-14|        1|2924468|  115720|       3.0|       null|null|  null|       null|       null|       null|     93.03|Quito|Pichincha|         D|     13|        1751|\n",
      "+----------+---------+-------+--------+----------+-----------+----+------+-----------+-----------+-----------+----------+-----+---------+----------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_holiday_oil_store_transaction_combine.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the file to a csv file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o100.collectToPython.\n: org.apache.spark.SparkException: Job 39 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2803)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2800)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2800)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2823)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2800)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b0ec3c0c8470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_holiday_oil_store_transaction_combine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_holiday_oil_store_transaction_without_test_002.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \"\"\"\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o100.collectToPython.\n: org.apache.spark.SparkException: Job 39 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2803)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2800)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2800)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2823)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2800)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54277)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/lib/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/lib/python3.6/socketserver.py\", line 696, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/serializers.py\", line 577, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_holiday_oil_store_transaction_combine.toPandas().to_csv('train_holiday_oil_store_transaction_without_test_002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

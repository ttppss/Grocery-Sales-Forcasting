{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SalesPrediction1026\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays_events = spark.read.csv(\"../holidays_events.csv\", header=True, inferSchema=True)\n",
    "items = spark.read.csv(\"../items.csv\", header=True, inferSchema=True)\n",
    "#oil = spark.read.csv(\"../oil.csv\", header=True, inferSchema=True)\n",
    "stores = spark.read.csv(\"../stores.csv\", header=True, inferSchema=True)\n",
    "test = spark.read.csv(\"../test.csv\", header=True, inferSchema=True)\n",
    "train = spark.read.csv(\"../train.csv\", header=True, inferSchema=True)\n",
    "#train_sample = train.sample(False, 0.05, 1)\n",
    "transactions = spark.read.csv(\"../transactions.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deal with oil dataframe in pandas with backfill, then transform it to spark dataframe\n",
    "#oil_pandas = pd.read_csv(\"../oil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"\"\"SET spark.sql.autoBroadcastJoinThreshold = -1\"\"\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+--------+-----------+------+\n",
      "| id|               date|store_nbr|item_nbr|onpromotion|source|\n",
      "+---+-------------------+---------+--------+-----------+------+\n",
      "|  0|2013-01-01 00:00:00|       25|  103665|       null| train|\n",
      "|  1|2013-01-01 00:00:00|       25|  105574|       null| train|\n",
      "|  2|2013-01-01 00:00:00|       25|  105575|       null| train|\n",
      "|  3|2013-01-01 00:00:00|       25|  108079|       null| train|\n",
      "|  4|2013-01-01 00:00:00|       25|  108701|       null| train|\n",
      "|  5|2013-01-01 00:00:00|       25|  108786|       null| train|\n",
      "|  6|2013-01-01 00:00:00|       25|  108797|       null| train|\n",
      "|  7|2013-01-01 00:00:00|       25|  108952|       null| train|\n",
      "|  8|2013-01-01 00:00:00|       25|  111397|       null| train|\n",
      "|  9|2013-01-01 00:00:00|       25|  114790|       null| train|\n",
      "| 10|2013-01-01 00:00:00|       25|  114800|       null| train|\n",
      "| 11|2013-01-01 00:00:00|       25|  115267|       null| train|\n",
      "| 12|2013-01-01 00:00:00|       25|  115611|       null| train|\n",
      "| 13|2013-01-01 00:00:00|       25|  115693|       null| train|\n",
      "| 14|2013-01-01 00:00:00|       25|  115720|       null| train|\n",
      "| 15|2013-01-01 00:00:00|       25|  115850|       null| train|\n",
      "| 16|2013-01-01 00:00:00|       25|  115891|       null| train|\n",
      "| 17|2013-01-01 00:00:00|       25|  115892|       null| train|\n",
      "| 18|2013-01-01 00:00:00|       25|  115894|       null| train|\n",
      "| 19|2013-01-01 00:00:00|       25|  119024|       null| train|\n",
      "+---+-------------------+---------+--------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine test set and train set together\n",
    "#add 'source' column to both train and test set so we can separate them after data cleaning\n",
    "train = train.withColumn('source', lit('train'))\n",
    "test = test.withColumn('source', lit('test'))\n",
    "train_without_target = train.select(\"id\", 'date', 'store_nbr', 'item_nbr', 'onpromotion', 'source')\n",
    "#use .union() to add them together\n",
    "train_test_set = train_without_target.union(test)\n",
    "train_test_set.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train and holiday_events dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+\n",
      "|               date|     id|store_nbr|item_nbr|onpromotion|source|type|locale|locale_name|description|transferred|\n",
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+\n",
      "|2013-05-06 00:00:00|5202345|        1|  103520|       null| train|null|  null|       null|       null|       null|\n",
      "|2013-05-06 00:00:00|5202346|        1|  105574|       null| train|null|  null|       null|       null|       null|\n",
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_holiday = train_test_set.join(holidays_events, 'date', 'left_outer')\n",
    "train_holiday.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: timestamp, store_nbr: int, item_nbr: int, onpromotion: boolean, source: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Oil Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  dcoilwtico\n",
       "0  2013-01-01       93.14\n",
       "1  2013-01-02       93.14\n",
       "2  2013-01-03       92.97\n",
       "3  2013-01-04       93.12\n",
       "4  2013-01-07       93.20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oil_pandas = oil_pandas.fillna(method='bfill')\n",
    "# oil_pandas = oil_pandas.fillna(method='ffill')\n",
    "# oil_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          0\n",
       "dcoilwtico    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oil_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform pandas dataframe back to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|     93.14|\n",
      "|2013-01-02|     93.14|\n",
      "|2013-01-03|     92.97|\n",
      "|2013-01-04|     93.12|\n",
      "|2013-01-07|      93.2|\n",
      "|2013-01-08|     93.21|\n",
      "|2013-01-09|     93.08|\n",
      "|2013-01-10|     93.81|\n",
      "|2013-01-11|      93.6|\n",
      "|2013-01-14|     94.27|\n",
      "|2013-01-15|     93.26|\n",
      "|2013-01-16|     94.28|\n",
      "|2013-01-17|     95.49|\n",
      "|2013-01-18|     95.61|\n",
      "|2013-01-21|     96.09|\n",
      "|2013-01-22|     96.09|\n",
      "|2013-01-23|     95.06|\n",
      "|2013-01-24|     95.35|\n",
      "|2013-01-25|     95.15|\n",
      "|2013-01-28|     95.95|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oil_spark = spark.createDataFrame(oil_pandas)\n",
    "# oil_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'), ('dcoilwtico', 'double')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oil_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+----------+\n",
      "|               date|     id|store_nbr|item_nbr|onpromotion|source|type|locale|locale_name|description|transferred|dcoilwtico|\n",
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+----------+\n",
      "|2013-05-06 00:00:00|5202345|        1|  103520|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202346|        1|  105574|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202347|        1|  105575|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202348|        1|  105577|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202349|        1|  105693|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202350|        1|  105857|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202351|        1|  106716|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202352|        1|  108079|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202353|        1|  108696|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202354|        1|  108698|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202355|        1|  108701|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202356|        1|  108786|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202357|        1|  111223|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202358|        1|  111397|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202359|        1|  114778|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202360|        1|  114790|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202361|        1|  114799|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202362|        1|  114800|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202363|        1|  115267|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "|2013-05-06 00:00:00|5202364|        1|  115611|       null| train|null|  null|       null|       null|       null|      95.8|\n",
      "+-------------------+-------+---------+--------+-----------+------+----+------+-----------+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_holiday_oil = train_holiday.join(oil_spark, 'date', 'left_outer')\n",
    "# train_holiday_oil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, id: int, store_nbr: int, item_nbr: int, onpromotion: boolean, source: string, type: string, locale: string, locale_name: string, description: string, transferred: boolean]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_holiday.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train_holiday_oil with store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename store.type so it would not duplicate with holiday.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------+--------+-----------+------+----+------+-----------+-----------+-----------+--------+--------+----------+-------+\n",
      "|store_nbr|               date|      id|item_nbr|onpromotion|source|type|locale|locale_name|description|transferred|    city|   state|store_type|cluster|\n",
      "+---------+-------------------+--------+--------+-----------+------+----+------+-----------+-----------+-----------+--------+--------+----------+-------+\n",
      "|       31|2013-05-06 00:00:00| 5226034|  103501|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164509| 1464035|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226035|  103520|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164510| 1464066|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226036|  103665|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164511| 1464070|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226037|  105576|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164512| 1464071|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226038|  105693|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164513| 1464073|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226039|  106716|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164514| 1464076|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226040|  108079|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164515| 1464079|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226041|  108698|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164516| 1464081|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226042|  108786|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164517| 1464082|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2013-05-06 00:00:00| 5226043|  108797|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "|       31|2014-01-30 00:00:00|18164518| 1464086|       null| train|null|  null|       null|       null|       null|Babahoyo|Los Rios|         B|     10|\n",
      "+---------+-------------------+--------+--------+-----------+------+----+------+-----------+-----------+-----------+--------+--------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores = stores.withColumnRenamed(\"type\", \"store_type\")\n",
    "train_holiday_oil_store = train_holiday.join(stores, 'store_nbr', 'left_outer')\n",
    "train_holiday_oil_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, id: int, store_nbr: int, item_nbr: int, onpromotion: boolean, source: string, type: string, locale: string, locale_name: string, description: string, transferred: boolean, dcoilwtico: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_holiday_oil.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join train_holiday_oil_store and transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#try to avoid the duplicated column name:\n",
    "train_holiday_oil_store_transaction = train_holiday_oil_store.join(transactions, ['date', 'store_nbr'], 'left_outer')\n",
    "train_holiday_oil_store_transaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join items table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item = train_holiday_oil_store_transaction.join(items, 'item_nbr', 'left_outer')\n",
    "#train_holiday_oil_store_transaction_item.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Created a new dataframe called test, so I would not damage the previous one. \n",
    "train_holiday_oil_store_transaction_item_test = train_holiday_oil_store_transaction_item.fillna({\"onpromotion\": False})\n",
    "#train_holiday_oil_store_transaction_item_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_holiday_oil_store_transaction_item_test_003[train_holiday_oil_store_transaction_item_test_003['source'] == 'test'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a table for on_promotion, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# onpromotions = train_holiday_oil_store_transaction_item_test.select(\"onpromotion\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# exprs = [F.when(F.col(\"onpromotion\") == onpromotion, 1).otherwise(0).alias('onpromotion_' + onpromotion)\n",
    "#          for onpromotion in onpromotions]\n",
    "\n",
    "#train_holiday_oil_store_transaction_item_test.select(\"item_nbr\", *exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#item table with onpromotion encoded to 1 and 0\n",
    "# item_onpromotion = train_holiday_oil_store_transaction_item_test.select(\"item_nbr\", *exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode item_family and create a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"family\", outputCol=\"familyIndex\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"familyIndex\", outputCol=\"familyIndexVec\")\n",
    "train_holiday_oil_store_transaction_item_test = encoder.transform(indexed)\n",
    "\n",
    "train_holiday_oil_store_transaction_item_test = train_holiday_oil_store_transaction_item_test.drop('family')\n",
    "#family_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"city\", outputCol=\"cityIndex\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"cityIndex\", outputCol=\"cityIndexVec\")\n",
    "train_holiday_oil_store_transaction_item_test = encoder.transform(indexed)\n",
    "\n",
    "train_holiday_oil_store_transaction_item_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test = train_holiday_oil_store_transaction_item_test.drop('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"cluster\", outputCol=\"clusterIndex\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"clusterIndex\", outputCol=\"clusterIndexVec\")\n",
    "train_holiday_oil_store_transaction_item_test = encoder.transform(indexed)\n",
    "\n",
    "train_holiday_oil_store_transaction_item_test = train_holiday_oil_store_transaction_item_test.drop('cluster')\n",
    "train_holiday_oil_store_transaction_item_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with holiday_event dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay attention to how I fillna with a bolean value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill NaN value in transferred column:\n",
    "train_holiday_oil_store_transaction_item_test_002 = train_holiday_oil_store_transaction_item_test.fillna('False', subset=['type', 'locale', 'locale_name', 'description'])\n",
    "train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_002.fillna({\"transferred\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003[train_holiday_oil_store_transaction_item_test_003['source'] == 'test'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test_003)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test_003)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"typeIndex\", outputCol=\"typeIndexVec\")\n",
    "train_holiday_oil_store_transaction_item_test_003 = encoder.transform(indexed)\n",
    "\n",
    "train_holiday_oil_store_transaction_item_test_003.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"locale_name\", outputCol=\"locale_nameIndex\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test_003)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test_003)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"locale_nameIndex\", outputCol=\"locale_nameIndexVec\")\n",
    "train_holiday_oil_store_transaction_item_test_003 = encoder.transform(indexed)\n",
    "\n",
    "train_holiday_oil_store_transaction_item_test_003.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all dataframes together, then drop original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_003.drop('type', 'description', 'transferred', 'state', 'locale', 'locale_name', 'family', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cast bolean values to int\n",
    "train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_003.withColumn(\"onpromotion\", train_holiday_oil_store_transaction_item_test_003[\"onpromotion\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer = StringIndexer(inputCol=\"onpromotion\", outputCol=\"onpromotionIndex\")\n",
    "# model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test_003)\n",
    "# indexed = model.transform(train_holiday_oil_store_transaction_item_test_003)\n",
    "\n",
    "# encoder = OneHotEncoder(inputCol=\"onpromotionIndex\", outputCol=\"onpromotionIndexVec\")\n",
    "# train_holiday_oil_store_transaction_item_test_004 = encoder.transform(indexed)\n",
    "\n",
    "# train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('onpromotion')\n",
    "# train_holiday_oil_store_transaction_item_test_004.show()\n",
    "train_holiday_oil_store_transaction_item_test_003.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fillna for onpromation:\n",
    "train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_003.fillna(0, subset=['onpromotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003[train_holiday_oil_store_transaction_item_test_003.onpromotion.isNull()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test.unpersist()\n",
    "train_holiday_oil_store_transaction_item_test_002.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with store_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"store_type\", outputCol=\"store_type_Index\")\n",
    "model = stringIndexer.fit(train_holiday_oil_store_transaction_item_test_003)\n",
    "indexed = model.transform(train_holiday_oil_store_transaction_item_test_003)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"store_type_Index\", outputCol=\"store_type_Index_Vec\")\n",
    "train_holiday_oil_store_transaction_item_test_003 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_003.drop('store_type')\n",
    "train_holiday_oil_store_transaction_item_test_003.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_holiday_oil_store_transaction_item_test_003.where(train_holiday_oil_store_transaction_item_test_003['source'] == 'test').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  (drop NaN oil temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#without oil column\n",
    "train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_003.drop('dcoilwtico')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separate Train set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_001 = train_holiday_oil_store_transaction_item_test_004.filter(train_holiday_oil_store_transaction_item_test_004.source == 'train')\n",
    "test_001 = train_holiday_oil_store_transaction_item_test_004.filter(train_holiday_oil_store_transaction_item_test_004.source == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = train.select('id', 'unit_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_001 = train_001.join(train_target, 'id', 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_001 = train_001.drop('transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_001.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_001.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_001 = test_001.drop('transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_001.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_001.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['store_nbr', 'perishable', 'familyIndex', \\\n",
    "                             'cityIndex',  'clusterIndex',  'typeIndex', \\\n",
    "                             'locale_nameIndex',  \\\n",
    "                              'store_type_Index']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_train = assembler.transform(train_001)\n",
    "assembled_train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = assembled_train.randomSplit([0.6, 0.4], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10).setLabelCol(\"unit_sales\").setFeaturesCol(\"features\")\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_summary = model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0, 0.0, 13.0, 8.0, 1.0, 1.0, 0.0),\n",
       " (25, 0, 2.0, 13.0, 8.0, 1.0, 1.0, 0.0),\n",
       " (25, 0, 0.0, 13.0, 8.0, 1.0, 1.0, 0.0),\n",
       " (25, 0, 0.0, 13.0, 8.0, 1.0, 1.0, 0.0),\n",
       " (1, 0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = train_004.rdd.map(lambda row: row[1:])\n",
    "# features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardizer = StandardScaler()\n",
    "# model = standardizer.fit(features)\n",
    "# features_transform = model.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0]),\n",
       " DenseVector([1.531, 0.0, 0.4524, 2.2903, 1.8854, 1.2746, 0.2836, 0.0]),\n",
       " DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0]),\n",
       " DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0]),\n",
       " DenseVector([0.0612, 0.0, 0.0, 0.0, 0.9427, 0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_transform.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 4.0, 2.0, 1.0, 2.0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lab = train_004.rdd.map(lambda row: row[0])\n",
    "# lab.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0])),\n",
       " (4.0, DenseVector([1.531, 0.0, 0.4524, 2.2903, 1.8854, 1.2746, 0.2836, 0.0])),\n",
       " (2.0, DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0])),\n",
       " (1.0, DenseVector([1.531, 0.0, 0.0, 2.2903, 1.8854, 1.2746, 0.2836, 0.0])),\n",
       " (2.0, DenseVector([0.0612, 0.0, 0.0, 0.0, 0.9427, 0.0, 0.0, 0.0]))]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformedData = lab.zip(features_transform)\n",
    "# transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformedData = transformedData.map(lambda row: LabeledPoint(row[0],[row[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainingData, testingData = transformedData.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# linearModel = LinearRegressionWithSGD.train(trainingData,1000,.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(6.0, [0.122476554263,2.30139302132,2.48797285662,0.0,0.942720841619,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [0.244953108527,0.0,0.452358701203,0.0,2.35680210405,0.0,0.0,0.0]),\n",
       " LabeledPoint(4.0, [0.36742966279,0.0,0.0,0.0,0.942720841619,0.0,0.0,0.0]),\n",
       " LabeledPoint(14.0, [0.428667939922,0.0,0.226179350602,0.0,1.17840105202,0.0,0.0,0.0]),\n",
       " LabeledPoint(6.0, [0.428667939922,0.0,0.0,0.0,1.17840105202,0.0,0.0,0.0]),\n",
       " LabeledPoint(10.0, [0.428667939922,2.30139302132,0.678538051805,0.0,1.17840105202,0.0,0.0,0.0]),\n",
       " LabeledPoint(15.0, [0.489906217053,0.0,0.226179350602,0.0,1.17840105202,0.0,0.0,0.0]),\n",
       " LabeledPoint(5.0, [0.551144494185,0.0,0.0,0.0,0.0,0.0,0.0,2.34845491678]),\n",
       " LabeledPoint(1.0, [0.551144494185,0.0,0.452358701203,0.0,0.0,0.0,0.0,2.34845491678]),\n",
       " LabeledPoint(2.0, [0.612382771316,0.0,0.0,0.0,1.41408126243,0.0,0.0,1.56563661119])]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testingData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Model with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0006941816329661954"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.mllib.evaluation import RegressionMetrics\n",
    "# prediObserRDDin = trainingData.map(lambda row: (float(linearModel.predict(row.features[0])),row.label))\n",
    "# metrics = RegressionMetrics(prediObserRDDin)\n",
    "# metrics.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_003 = test_002.select('store_nbr', 'perishable', 'familyIndex', \\\n",
    "#                              'cityIndex',  'clusterIndex',  'typeIndex', \\\n",
    "#                              'locale_nameIndex',  \\\n",
    "#                               'store_type_Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_test = test_003.rdd.map(lambda row: row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0, 12.0, 2.0, 0.0, 0.0, 4.0),\n",
       " (0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0),\n",
       " (0, 0.0, 4.0, 1.0, 0.0, 0.0, 1.0),\n",
       " (0, 0.0, 14.0, 3.0, 1.0, 3.0, 2.0),\n",
       " (0, 0.0, 2.0, 11.0, 0.0, 0.0, 0.0)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_test.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_test = standardizer.fit(features_test)\n",
    "# features_test_transform = model_test.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 0.0, 1.901, 0.4653, 0.0, 0.0, 3.1083]),\n",
       " DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3312]),\n",
       " DenseVector([0.0, 0.0, 0.6337, 0.2327, 0.0, 0.0, 0.7771]),\n",
       " DenseVector([0.0, 0.0, 2.2179, 0.698, 4.1312, 4.1312, 1.5542]),\n",
       " DenseVector([0.0, 0.0, 0.3168, 2.5594, 0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_test_transform.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #prediObserRDDin = trainingData.map(lambda row: (float(linearModel.predict(row.features[0])),row.label))\n",
    "# prediction_test = features_test_transform.map(lambda row: linearModel.predict(float(row.features_test[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1847.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1847.0 (TID 63149, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-160-9b4d5ee208bd>\", line 2, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 481, in __getattr__\n    return getattr(self.array, item)\nAttributeError: 'numpy.ndarray' object has no attribute 'features_test'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-160-9b4d5ee208bd>\", line 2, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 481, in __getattr__\n    return getattr(self.array, item)\nAttributeError: 'numpy.ndarray' object has no attribute 'features_test'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-9b0fe055c118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1847.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1847.0 (TID 63149, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-160-9b4d5ee208bd>\", line 2, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 481, in __getattr__\n    return getattr(self.array, item)\nAttributeError: 'numpy.ndarray' object has no attribute 'features_test'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-160-9b4d5ee208bd>\", line 2, in <lambda>\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 481, in __getattr__\n    return getattr(self.array, item)\nAttributeError: 'numpy.ndarray' object has no attribute 'features_test'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# prediction_test.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# lr = LogisticRegression(labelCol=\"lab\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-3d279c67d2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "# lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
